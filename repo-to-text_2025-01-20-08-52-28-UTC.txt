Directory: awesome-whisper-finetuning

Directory Structure:
```
.
├── .gitignore
├── .env.template
├── LICENSE
├── Makefile
│   │   ├── apps/data_curation/__init__.py
│   │   ├── apps/data_curation/main.py
│   │   ├── apps/data_curation/prompts
│   │   │   ├── apps/data_curation/prompts/__init__.py
│   │   │   └── apps/data_curation/prompts/v1
│   │   │       ├── apps/data_curation/prompts/v1/__init__.py
│   │   └── apps/data_curation/utils
│   │       ├── apps/data_curation/utils/__init__.py
│   │       ├── apps/data_curation/utils/data_preparation.py
│   │       ├── apps/data_curation/utils/db.py
│   │       ├── apps/data_curation/utils/language_mapping.py
│   │       ├── apps/data_curation/utils/llm_client.py
│   │       └── apps/data_curation/utils/tts_client.py
│   ├── apps/serving
│   │   ├── apps/serving/__init__.py
│   │   ├── apps/serving/main.py
│   │   └── apps/serving/utils
│   │       └── apps/serving/utils/inference_utils.py
│   └── apps/training
│       ├── apps/training/__init__.py
│       ├── apps/training/main.py
│       └── apps/training/utils
│           └── apps/training/utils/training_utils.py
│   ├── docker/Dockerfile.data_curation
│   ├── docker/Dockerfile.serving
│   └── docker/Dockerfile.training
│   ├── scripts/generate_manifests.py
│   └── scripts/setup_k3d.sh
    │   └── shared/clients/api_client.py
    ├── shared/config
    │   └── shared/config/app_config.py
    └── shared/utils
        └── shared/utils/logger.py
```

Contents of LICENSE:
```
MIT License

Copyright (c) 2025 qilipuAI

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

Contents of Makefile:
```
.PHONY: data_curation training serving docker-build docker-run docker-stop docker-clean compose-build compose-run compose-stop compose-clean deploy

# Loading environment variables
include .env
export $(shell sed 's/=.*//' .env)

# Local Environment
# Run Data Curation API
data_curation:
	poetry run uvicorn apps.data_curation.main:app --host 0.0.0.0 --port 8001 --reload

# Run Training API
training:
	poetry run uvicorn apps.training.main:app --host 0.0.0.0 --port 8002 --reload

# Run Serving API
serving:
	poetry run uvicorn apps.serving.main:app --host 0.0.0.0 --port 8003 --reload

# Docker Environment with Docker Compose
docker-build:
	docker-compose build

docker-run:
	docker-compose up -d

docker-stop:
	docker-compose stop

docker-clean:
	docker-compose down -v

# Kubernetes (k3d) Environment
k3d-setup:
	./scripts/setup_k3d.sh

k3d-deploy:
	kubectl apply -f k8s/storage.yaml
	kubectl apply -f k8s/data-curation.yaml
	kubectl apply -f k8s/training.yaml
	kubectl apply -f k8s/serving.yaml

k3d-clean:
	kubectl delete -f k8s/serving.yaml
	kubectl delete -f k8s/training.yaml
	kubectl delete -f k8s/data-curation.yaml
	kubectl delete -f k8s/storage.yaml
	k3d cluster delete whisper-finetuning-cluster

# Deployment to Kubernetes
deploy:
	./scripts/deploy.sh

# Shortcut Targets
compose-build: docker-build
compose-run: docker-run
compose-stop: docker-stop
compose-clean: docker-clean

# General Cleanup
clean: compose-clean k3d-clean
	docker system prune -a -f --volumes

```

Contents of .env.template:
```
# Google API Configuration
GOOGLE_API_KEY=your_api_key_here

# Other sensitive configurations can go here
# DATABASE_URL=
# OTHER_API_KEYS= 
```

Contents of docker/Dockerfile.serving:
```
FROM python:3.11-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Rust for building dependencies
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH=/root/.cargo/bin:$PATH

# Install Poetry
ENV POETRY_HOME=/opt/poetry
ENV POETRY_VIRTUALENVS_IN_PROJECT=1

RUN curl -sSL https://install.python-poetry.org | python3 - \
    && ln -s /opt/poetry/bin/poetry /usr/local/bin/poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies without dev dependencies
RUN poetry install --no-interaction --no-ansi --no-root

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Copy only runtime dependencies from builder
COPY --from=builder /app/.venv /app/.venv
ENV PATH="/app/.venv/bin:$PATH"

# Copy only necessary source code
COPY ./apps/serving ./apps/serving
COPY ./shared ./shared

# Install runtime dependencies and cleanup in same layer
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
       ffmpeg \
    && rm -rf /var/lib/apt/lists/*

EXPOSE 8003
CMD ["uvicorn", "apps.serving.main:app", "--host", "0.0.0.0", "--port", "8003"]

```

Contents of docker/Dockerfile.data_curation:
```
FROM python:3.11-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Rust for building dependencies
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH=/root/.cargo/bin:$PATH

# Install Poetry
ENV POETRY_HOME=/opt/poetry
ENV POETRY_VIRTUALENVS_IN_PROJECT=1
ENV COQUI_TTS_AGREED=1

RUN curl -sSL https://install.python-poetry.org | python3 - \
    && ln -s /opt/poetry/bin/poetry /usr/local/bin/poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies
RUN poetry install --no-interaction --no-ansi --no-root

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Copy only runtime dependencies from builder
COPY --from=builder /app/.venv /app/.venv
ENV PATH="/app/.venv/bin:$PATH"

# Copy only necessary source code
COPY ./apps/data_curation ./apps/data_curation
COPY ./shared ./shared

# Create data directories and cleanup in same layer
RUN mkdir -p /app/data/assets \
    && apt-get update \
    && apt-get install -y --no-install-recommends \
       ffmpeg \
    && rm -rf /var/lib/apt/lists/*

EXPOSE 8001

CMD ["uvicorn", "apps.data_curation.main:app", "--host", "0.0.0.0", "--port", "8001"]

```

Contents of docker/Dockerfile.training:
```
FROM python:3.11-slim as builder

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Rust for building dependencies
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH=/root/.cargo/bin:$PATH

# Install Poetry
ENV POETRY_HOME=/opt/poetry
ENV POETRY_VIRTUALENVS_IN_PROJECT=1

RUN curl -sSL https://install.python-poetry.org | python3 - \
    && ln -s /opt/poetry/bin/poetry /usr/local/bin/poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies without dev dependencies
RUN poetry config virtualenvs.in-project true \
    && poetry install --no-interaction --no-ansi --no-root

# Final stage
FROM python:3.11-slim

WORKDIR /app

# Copy only runtime dependencies from builder
COPY --from=builder /app/.venv /app/.venv
ENV PATH="/app/.venv/bin:$PATH"

# Copy only necessary source code
COPY ./apps/training ./apps/training
COPY ./shared ./shared

EXPOSE 8002
CMD ["uvicorn", "apps.training.main:app", "--host", "0.0.0.0", "--port", "8002"]

```

Contents of shared/clients/api_client.py:
```
import httpx
from typing import Optional, Dict, Any
import os


class WhisperAPIClient:
    def __init__(self):
        self.data_curation_url = os.getenv(
            "DATA_CURATION_URL", 
            "http://data-curation:8000"
        )
        self.training_url = os.getenv("TRAINING_URL", "http://training:8000")
        self.serving_url = os.getenv("SERVING_URL", "http://serving:8000")
        self.client = httpx.AsyncClient(timeout=300.0)  # 5 minutes timeout
    
    async def generate_data(
        self,
        language: str,
        scenario: str,
        character: str,
        request: str,
        n_sample: int,
        tone: str
    ) -> Dict[str, Any]:
        """Generate training data using the data curation service."""
        response = await self.client.post(
            f"{self.data_curation_url}/generate",
            json={
                "language": language,
                "scenario": scenario,
                "character": character,
                "request": request,
                "nSample": n_sample,
                "tone": tone
            }
        )
        response.raise_for_status()
        return response.json()
    
    async def train_model(
        self,
        dataset_path: str,
        model_name: str,
        output_dir: str,
        language_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """Start model training using the training service."""
        response = await self.client.post(
            f"{self.training_url}/train",
            json={
                "dataset_path": dataset_path,
                "model_name": model_name,
                "output_dir": output_dir,
                "language_filter": language_filter
            }
        )
        response.raise_for_status()
        return response.json()
    
    async def transcribe_audio(
        self,
        audio_path: str,
        model_dir: str,
        language_code: str
    ) -> Dict[str, Any]:
        """Transcribe audio using the serving service."""
        response = await self.client.post(
            f"{self.serving_url}/transcribe",
            json={
                "audio_path": audio_path,
                "model_dir": model_dir,
                "language_code": language_code
            }
        )
        response.raise_for_status()
        return response.json()
    
    async def close(self):
        """Close the HTTP client."""
        await self.client.aclose() 
```

Contents of shared/config/app_config.py:
```
from pathlib import Path

class AppConfig:
    # Base paths
    BASE_DIR = Path(__file__).parent.parent.parent
    DATA_DIR = BASE_DIR / "data"
    
    # Database configuration
    DB_PATH = DATA_DIR / "assets" / "scenarios.db"
    
    # Data directories
    ASSETS_DIR = DATA_DIR / "assets"
    DATASETS_DIR = DATA_DIR / "datasets"
    TRAINING_DATA_DIR = DATA_DIR / "training_data"
    
    # Prompts configuration
    PROMPTS_DIR = BASE_DIR / "apps" / "data_curation" / "prompts"
    
    # Model configurations
    LLM_MODEL = "gemini-2.0-flash-exp"
    TTS_MODEL = "tts_models/multilingual/multi-dataset/xtts_v2"
    
    # API configurations
    TRAINING_SERVICE_TITLE = "Training Service"
    SERVING_SERVICE_TITLE = "Serving Service"
    DATA_CURATION_SERVICE_TITLE = "Data Curation Service"
    API_VERSION = "1.0"

config = AppConfig()

```

Contents of shared/utils/logger.py:
```
import logging

def setup_logging():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
```

Contents of scripts/generate_manifests.py:
```
#!/usr/bin/env python3
import yaml
import os
from pathlib import Path

def load_config():
    """Load repository configuration with environment variable substitution."""
    config_path = Path(__file__).parent.parent / "config" / "repository.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)
    
    # Substitute environment variables
    registry = os.getenv("CONTAINER_REGISTRY", config["container_registry"]["host"])
    namespace = os.getenv("REGISTRY_NAMESPACE", config["container_registry"]["namespace"])
    tag = os.getenv("IMAGE_TAG", config["container_registry"]["tag"])
    
    return {
        "registry": registry,
        "namespace": namespace,
        "tag": tag,
        "services": config["services"]
    }

def generate_image_name(registry, namespace, service, tag):
    """Generate full image name."""
    return f"{registry}/{namespace}/{service}:{tag}"

def generate_manifests(config):
    """Generate Kubernetes manifests for all services."""
    k8s_dir = Path(__file__).parent.parent / "k8s"
    k8s_dir.mkdir(exist_ok=True)
    
    # Generate manifests for each service
    for service in config["services"]:
        name = service["name"]
        image = generate_image_name(
            config["registry"],
            config["namespace"],
            name,
            config["tag"]
        )
        
        if name == "training":
            manifest = generate_training_manifest(name, image)
        else:
            manifest = generate_service_manifest(
                name,
                image,
                service["port"],
                service["target_port"]
            )
        
        # Write manifest to file
        manifest_path = k8s_dir / f"{name}.yaml"
        with open(manifest_path, "w") as f:
            yaml.dump(manifest, f)

def generate_service_manifest(name, image, port, target_port):
    """Generate manifest for regular services."""
    return {
        "apiVersion": "apps/v1",
        "kind": "Deployment",
        "metadata": {"name": name},
        "spec": {
            "replicas": 2 if name == "serving" else 1,
            "selector": {"matchLabels": {"app": name}},
            "template": {
                "metadata": {"labels": {"app": name}},
                "spec": {
                    "containers": [{
                        "name": name,
                        "image": image,
                        "resources": {
                            "limits": {"nvidia.com/gpu": 1}
                        } if name == "serving" else {},
                        "ports": [{"containerPort": target_port}],
                        "env": [
                            {"name": "DB_PATH", "value": "/data/scenarios.db"}
                        ] if name == "data-curation" else [
                            {"name": "MODEL_DIR", "value": "/data/whisper_finetuned"}
                        ] if name == "serving" else [],
                        "volumeMounts": [{
                            "name": "data-volume",
                            "mountPath": "/data"
                        }]
                    }],
                    "volumes": [{
                        "name": "data-volume",
                        "persistentVolumeClaim": {"claimName": "data-pvc"}
                    }]
                }
            }
        }
    }

def generate_training_manifest(name, image):
    """Generate manifest for training job."""
    return {
        "apiVersion": "batch/v1",
        "kind": "Job",
        "metadata": {"name": "whisper-training"},
        "spec": {
            "template": {
                "spec": {
                    "containers": [{
                        "name": name,
                        "image": image,
                        "resources": {
                            "limits": {"nvidia.com/gpu": 1}
                        },
                        "env": [
                            {"name": "DATASET_PATH", "value": "/data/training_data/training_data.json"},
                            {"name": "MODEL_NAME", "value": "openai/whisper-small"},
                            {"name": "OUTPUT_DIR", "value": "/data/whisper_finetuned"}
                        ],
                        "volumeMounts": [{
                            "name": "data-volume",
                            "mountPath": "/data"
                        }]
                    }],
                    "volumes": [{
                        "name": "data-volume",
                        "persistentVolumeClaim": {"claimName": "data-pvc"}
                    }],
                    "restartPolicy": "Never"
                }
            },
            "backoffLimit": 4
        }
    }

if __name__ == "__main__":
    config = load_config()
    generate_manifests(config)
    print("Kubernetes manifests generated successfully!") 
```

Contents of scripts/setup_k3d.sh:
```
#!/bin/bash

# Default values
export CONTAINER_REGISTRY=${CONTAINER_REGISTRY:-"ghcr.io"}
export REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE:-"local"}
export IMAGE_TAG=${IMAGE_TAG:-"latest"}
export GITHUB_OWNER=${GITHUB_OWNER:-"local"}
export GITHUB_REPO=${GITHUB_REPO:-"whisper-finetuning"}

# Create k3d cluster with GPU support
k3d cluster create whisper-cluster \
  --gpus all \
  --agents 2 \
  --k3s-arg '--node-label=gpu=true@agent:0' \
  --k3s-arg '--node-label=gpu=true@agent:1' \
  --volume /tmp/k3dvol:/var/lib/rancher/k3s/storage@all

# Install NVIDIA device plugin
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml

# Create namespace
kubectl create namespace whisper

# Generate Kubernetes manifests
python scripts/generate_manifests.py

# Apply manifests
kubectl apply -f k8s/storage.yaml
kubectl apply -f k8s/data-curation.yaml
kubectl apply -f k8s/training.yaml
kubectl apply -f k8s/serving.yaml

# Setup port forwarding in background
kubectl port-forward -n whisper svc/data-curation 8001:8000 &
kubectl port-forward -n whisper svc/training 8002:8000 &
kubectl port-forward -n whisper svc/serving 8003:8000 &

echo "Cluster setup complete!"
echo "Services are available at:"
echo "- Data Curation: http://localhost:8001"
echo "- Training: http://localhost:8002"
echo "- Serving: http://localhost:8003"
echo
echo "Use 'kubectl get pods -n whisper' to check status." 
```

Contents of apps/serving/__init__.py:
```

```

Contents of apps/serving/main.py:
```
from fastapi import FastAPI, HTTPException
from shared.utils.logger import setup_logging
from pydantic import BaseModel
from apps.serving.utils.inference_utils import transcribe_audio
from shared.config.app_config import config

# Initialize the app
app = FastAPI(title=config.SERVING_SERVICE_TITLE, version=config.API_VERSION)

class InferenceRequest(BaseModel):
    audio_path: str
    model_dir: str
    language_code: str
# Setup logging
setup_logging()

# Root endpoint for health check
@app.get("/")
async def health_check():
    return {"status": "Serving Service is running"}

@app.post("/transcribe")
async def transcribe(request: InferenceRequest):
    try:
        transcription = transcribe_audio(
            audio_path=request.audio_path,
            model_dir=request.model_dir,
            language_code=request.language_code
        )
        return {"status": "success", "transcription": transcription}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

Contents of apps/serving/utils/inference_utils.py:
```
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa

def load_model_and_processor(model_dir="whisper_finetuned"):
    """
    Load the fine-tuned Whisper model and processor.
    """
    print(f"Loading model and processor from {model_dir}...")
    processor = WhisperProcessor.from_pretrained(model_dir)
    model = WhisperForConditionalGeneration.from_pretrained(model_dir)
    model.eval()  # Set the model to evaluation mode
    return model, processor

def preprocess_audio(audio_path, processor, sampling_rate=16000):
    """
    Load and preprocess audio for Whisper inference.
    """
    print(f"Preprocessing audio: {audio_path}")
    audio, sr = librosa.load(audio_path, sr=sampling_rate)

    # Ask the processor to return the attention mask
    features = processor.feature_extractor(
        audio, 
        sampling_rate=sampling_rate, 
        return_tensors="pt", 
        return_attention_mask=True
    )
    return features

def transcribe_audio(audio_path, model_dir, language_code="en"):
    """
    Perform inference on a single audio file and generate transcription.
    """
    model, processor = load_model_and_processor(model_dir)
    # Preprocess the audio to get both input_features and attention_mask
    features = preprocess_audio(audio_path, processor)
    input_features = features["input_features"]
    attention_mask = features["attention_mask"]

    # Generate transcription
    print(f"Generating transcription for {audio_path}...")
    with torch.no_grad():
        generated_ids = model.generate(
            input_features=input_features,
            attention_mask=attention_mask,  # Pass the attention mask here
            forced_decoder_ids=processor.get_decoder_prompt_ids(
                language=language_code,
                task="transcribe"
            )
        )
    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return transcription

if __name__ == "__main__":
    model_dir = "data/whisper_finetuned"
    audio_path = "data/inference_audio/output1.wav"

    try:
        transcription = transcribe_audio(audio_path, model_dir, language_code="en")
        print(f"Transcription:\n{transcription}")
    except Exception as e:
        print(f"Error during inference: {e}")

```

Contents of apps/data_curation/__init__.py:
```

```

Contents of apps/data_curation/main.py:
```
from fastapi import FastAPI, HTTPException
from shared.utils.logger import setup_logging
from shared.config.app_config import config
from pydantic import BaseModel
from apps.data_curation.utils.llm_client import LLMClient
from apps.data_curation.utils.tts_client import TTSClient
from apps.data_curation.utils.data_preparation import prepare_whisper_data
from apps.data_curation.utils.db import create_sqlite_database
import os

# Initialize the app
app = FastAPI(title=config.DATA_CURATION_SERVICE_TITLE, version=config.API_VERSION)

# Initialize database on startup
create_sqlite_database(config.DB_PATH)  # This will create the DB if it doesn't exist

class GenerationRequest(BaseModel):
    language: str
    scenario: str
    character: str
    request: str
    nSample: int  
    tone: str


# Setup logging
setup_logging()


# Root endpoint for health check
@app.get("/")
async def health_check():
    return {"status": "Data Curation Service is running"}

@app.post("/generate")
async def generate_data(request: GenerationRequest):
    try:
        # Step 1: Generate the next version for the dataset
        
        version = "v1"
        
        # Step 2: Generate conversation scripts
        llm = LLMClient(model=config.LLM_MODEL, prompt_version=version, db_path=config.DB_PATH)
        prompts_path = config.PROMPTS_DIR / version
        utterance_ids = llm.generate_conversations(
            request.dict(),
            prompt_path=prompts_path,
            output_dir=config.DATASETS_DIR
        )
        
        # Step 3: Generate audio files
        tts = TTSClient(tts_model=config.TTS_MODEL, db_path=config.DB_PATH)
        tts.generate_audio(
            utterance_ids,
            config.ASSETS_DIR / "test_audio.wav"
        )

        # Step 4: Prepare Whisper data
        whisper_data_path = prepare_whisper_data(config.DB_PATH, output_dir=config.TRAINING_DATA_DIR)

        return {"status": "success", "data_version": version, "whisper_data": str(whisper_data_path)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


```

Contents of apps/data_curation/utils/db.py:
```
import sqlite3
import os
import hashlib

import sqlite3

def get_db_connection(db_path):
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    return conn


def create_sqlite_database(db_filename="data/assets/scenarios.db"):
    """
    Creates a SQLite database with the necessary schema for storing data.

    Args:
        db_filename (str): The name or path of the database file. Defaults to "data/assets/scenarios.db".
    """
    conn = None
    try:
        # Ensure the directory exists for the database file
        os.makedirs(os.path.dirname(db_filename), exist_ok=True)

        # Check if the database file already exists
        if os.path.exists(db_filename):
            print(f"Database file '{db_filename}' already exists.")
            return

        # Create a connection to the database (this creates the file if it doesn't exist)
        conn = sqlite3.connect(db_filename)

        # Create a cursor object to execute SQL queries
        cursor = conn.cursor()

        # Create the single table 'dataset' with the required schema
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS scenarios (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scenario TEXT NOT NULL,
                scenario_id TEXT NOT NULL,
                language TEXT NOT NULL,
                language_code TEXT NOT NULL,
                utterance TEXT NOT NULL,
                audio_path TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(scenario_id, utterance)
            )
        ''')

        # Commit the changes to save the table creation
        conn.commit()

        print(f"Database file '{db_filename}' created successfully with the required schema.")

    except sqlite3.Error as e:
        print(f"An error occurred while creating the database: {e}")
    finally:
        # Ensure the database connection is always closed
        if conn:
            conn.close()


def hash_scenario_text(scenario_text):
    """
    Generates a unique hash for a given scenario text.

    Args:
        scenario_text (str): The scenario text to hash.

    Returns:
        str: A unique hash string.
    """
    return hashlib.sha256(scenario_text.encode()).hexdigest()[:16]  # 16-character hash


if __name__ == "__main__":
    # Define the default database file path
    db_file = "data/assets/scenarios.db"

    # Create the database with the required schema
    create_sqlite_database(db_file)

    # Example: Hash a scenario text
    sample_scenario = "A customer enters a small bookstore on a rainy day."
    print(f"Hash for scenario: {hash_scenario_text(sample_scenario)}")

```

Contents of apps/data_curation/utils/__init__.py:
```

```

Contents of apps/data_curation/utils/language_mapping.py:
```
# language_mapping.py

# Define the supported languages and their mappings
LANGUAGE_MAPPING = {
    "en": "english",
    "es": "spanish",
    "fr": "french",
    "de": "german",
    "it": "italian",
    "pt": "portuguese",
    "pl": "polish",
    "tr": "turkish",
    "ru": "russian",
    "nl": "dutch",
    "cs": "czech",
    "ar": "arabic",
    "zh-cn": "chinese",
    "hu": "hungarian",
    "ko": "korean",
    "ja": "japanese",
    "hi": "hindi"
}

# Reverse the mapping for lookup by full name
FULL_NAME_TO_CODE = {v: k for k, v in LANGUAGE_MAPPING.items()}

def get_language_code(language_name):
    """
    Get the language code for a given language name.
    :param language_name: Full language name (case insensitive) or language code.
    :return: Language code (e.g., 'en') or None if not found.
    """
    language_name = language_name.lower().strip()
    
    # If it's already a language code, return it if valid
    if language_name in LANGUAGE_MAPPING:
        return language_name
    
    # Check the full name to code mapping
    return FULL_NAME_TO_CODE.get(language_name)

def is_supported_language(language_code):
    """
    Check if a given language code is supported.
    :param language_code: Language code to check.
    :return: True if supported, False otherwise.
    """
    return language_code in LANGUAGE_MAPPING



```

Contents of apps/data_curation/utils/llm_client.py:
```
import google.generativeai as genai
from google.generativeai.types import GenerationConfig
from dotenv import load_dotenv
import os
import json
from jinja2 import Template
import sqlite3
from apps.data_curation.utils.db import hash_scenario_text
from apps.data_curation.utils.language_mapping import get_language_code, is_supported_language

load_dotenv()


class LLMClient:
    def __init__(self, model="gemini-2.0-flash-exp", prompt_version="v1", db_path="data/assets/scenarios.db"):
        """
        Initializes the LLM client.
        :param model: The Gemini 2.0 model to use.
        :param prompt_version: The version of the prompts to load.
        :param db_path: Path to the SQLite database.
        """
        GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        genai.configure(api_key=GEMINI_API_KEY)
        self.model = genai.GenerativeModel(model)
        self.prompt_version = prompt_version
        self.db_path = db_path

    def _load_prompts(self, prompts_path):
        """
        Load system and user prompts from the specified versioned directory.
        :return: Tuple of (system_prompt, user_prompt).
        """
        #prompts_path = f"apps/data_curation/prompts/{self.prompt_version}/"
        print(f"Loading prompts from: {os.path.abspath(prompts_path)}")
        with open(f"{prompts_path}/system_prompt.txt", "r") as system_file:
            system_prompt = system_file.read()
        with open(f"{prompts_path}/user_prompt.txt", "r") as user_file:
            user_prompt = user_file.read()
        return system_prompt, user_prompt

    def generate_conversations(self, user_inputs, prompt_path, output_dir="data/assets"):
        """
        Generate conversations based on the provided user inputs and save to SQLite and file system.
        :param user_inputs: Dictionary containing the user-provided parameters.
        :return: List of unique hashes generated for the utterances.
        """
        # Load prompts
        system_prompt, user_prompt_template = self._load_prompts(prompt_path)

        # Render the user prompt using Jinja2 template
        template = Template(user_prompt_template)
        rendered_user_prompt = template.render(**user_inputs)

        # Combine system and user prompts
        final_prompt = f"{system_prompt}\n\nUser Prompt:\n{rendered_user_prompt}"

        # Define generation configuration
        generation_config = GenerationConfig(
            max_output_tokens=10000,
            temperature=0.7,
            top_p=0.9,
            response_mime_type="application/json"
        )

        # Call the Gemini 2.0 API
        response = self.model.generate_content(
            contents=final_prompt,
            generation_config=generation_config,
            safety_settings=[]
        )

        # Parse the output
        try:
            output = json.loads(response.text)
            print(json.dumps(output, indent=4))
        except json.JSONDecodeError:
            raise ValueError("Failed to parse JSON response from the LLM.")

        # Save data and generate unique hashes for each utterance
        utterance_hashes = self._save_data(output, user_inputs, output_dir)
        return utterance_hashes

    def _save_data(self, llm_output, user_inputs, output_dir):
        """
        Save generated data to SQLite and the file system with unique hashes for each utterance.
        :param llm_output: Output from the LLM client.
        :param language: Full language name (e.g., "Danish").
        :return: List of unique hashes for the utterances.
        """
        scenario_dir = os.path.join(output_dir, user_inputs["language"])
        os.makedirs(scenario_dir, exist_ok=True)

        utterance_hashes = []
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            for idx, output in enumerate(llm_output["outputs"]):
                utterance = output["utterance"]
                scenario_hash = hash_scenario_text(f"{user_inputs['language']}_{user_inputs['scenario']}_{utterance}_{idx}")
                audio_path = os.path.join(scenario_dir, f"{scenario_hash}_utterance.wav")
                # Normalize language input to code
                language_code = get_language_code(user_inputs['language'])
                if not language_code or not is_supported_language(language_code):
                    raise ValueError(f"Unsupported language: {user_inputs['language']}")

                try:
                    cursor.execute("""
                    INSERT INTO scenarios (scenario, scenario_id, language, language_code, utterance, audio_path)
                    VALUES (?, ?, ?, ?, ?, ?)
                    """, (user_inputs["scenario"], scenario_hash, user_inputs["language"], language_code, utterance, audio_path))
                except sqlite3.IntegrityError:
                    print(f"Duplicate entry skipped for utterance: {utterance}")
                    continue

                # Save utterance JSON
                script_path = os.path.join(scenario_dir, f"{scenario_hash}_utterance.json")
                with open(script_path, "w") as script_file:
                    json.dump(output, script_file, indent=4)

                utterance_hashes.append(scenario_hash)

            conn.commit()
        print(f"Data saved to SQLite and assets folder. Generated hashes: {utterance_hashes}")
        return utterance_hashes
        
        
if __name__ == "__main__":
    llm_client = LLMClient(prompt_version="v1")

    user_inputs = {
        "language": "English",
        "scenario": "A customer enters a small bookstore on a rainy day.",
        "character": "A friendly, middle-aged bookstore owner who loves to chat with customers.",
        "request": "Greet the customer and make them feel welcome.",
        "nSample": 2,
        "tone": "Warm and inviting"
    }

    try:
        result = llm_client.generate_conversations(user_inputs, prompts_path="apps/data_curation/prompts/v1/", output_dir="data/assets")
        print("Generated Conversations:")
        print(json.dumps(result, indent=4))
    except ValueError as e:
        print(f"Error: {e}")
'''
{
"language": "English",
"scenario": "A customer enters a small bookstore on a rainy day.",
"character": "A friendly, middle-aged bookstore owner who loves to chat with customers.",
"request": "Greet the customer and make them feel welcome.",
"nSample": 2,
"tone": "Warm and inviting"
}

{
"language": "English",
"scenario": "A customer enters a small bookstore on a sunny day.",
"character": "A friendly, middle-aged bookstore owner who does not like to chat with customers.",
"request": "Greet the customer and make them feel uncomfortable.",
"nSample": 2,
"tone": "neutral"
}
'''
```

Contents of apps/data_curation/utils/tts_client.py:
```
import torch
from TTS.api import TTS
import os
import sqlite3

from apps.data_curation.utils.language_mapping import is_supported_language


class TTSClient:
    def __init__(self, tts_model="tts_models/multilingual/multi-dataset/xtts_v2", db_path="data/assets/scenarios.db"):
        """
        Initialize the TTS client.
        :param tts_model: The TTS model name to use.
        :param db_path: Path to the SQLite database.
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tts = TTS(model_name=tts_model, progress_bar=False).to(self.device)
        self.db_path = db_path
        print(f"Using TTS model: {tts_model}")

    @staticmethod
    def list_available_models():
        """
        List all available TTS models.
        :return: List of model names.
        """
        models = TTS().list_models()
        print("Available TTS Models:")
        for model in models:
            print(f"- {model}")
        return models

    def _generate_single_audio(self, utterance, language_code, output_path, speaker_wav="data/assets/test_audio.wav"):
        """
        Generate a single audio file for a given utterance and language.
        :param utterance: The text of the utterance.
        :param language_code: The language code for the utterance.
        :param output_path: File path to save the audio.
        :param speaker_wav: Optional reference speaker audio for speaker adaptation.
        """
        # Validate language code
        if not is_supported_language(language_code):
            raise ValueError(f"Unsupported language code: {language_code}")

        # Ensure the parent directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Generate the audio file
        self.tts.tts_to_file(text=utterance, language=language_code, speaker_wav=speaker_wav, file_path=output_path)
        print(f"Generated audio for: {utterance} -> {output_path}")
        return output_path

    def generate_audio(self, hash_ids, speaker_wav="data/assets/test_audio.wav"):
        audio_files = []
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            for hash_id in hash_ids:
                cursor.execute("SELECT utterance, language_code, audio_path FROM scenarios WHERE scenario_id = ?", (hash_id,))
                row = cursor.fetchone()
                if not row:
                    print(f"No record found for hash ID: {hash_id}")
                    continue
                utterance, language_code, audio_path = row
                audio_files.append(self._generate_single_audio(utterance, language_code, 
                                                            audio_path, speaker_wav))
        return audio_files


if __name__ == "__main__":
    tts_client = TTSClient()

    # List available models (optional)
    # tts_client.list_available_models()

    # Define hash IDs to generate audio for
    hash_ids = ["228e1a089f4b063f","76c6d59d82159fa7"]

    # Generate audio for the given hash IDs
    audio_files = tts_client.generate_audio(hash_ids, speaker_wav="data/assets/test_audio.wav")

    print(f"Generated audio files: {audio_files}")

```

Contents of apps/data_curation/utils/data_preparation.py:
```
import sqlite3
import json
import os

def prepare_whisper_data(db_path, output_dir="data/training_data/"):
    """
    Prepare Whisper-compatible data from the database.
    :param db_path: Path to the SQLite database.
    :param output_dir: Directory to save the prepared dataset.
    :return: Path to the prepared JSON dataset.
    """
    # Connect to the database
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Query to fetch audio paths and transcriptions
    query = "SELECT audio_path, utterance, language_code FROM scenarios"
    cursor.execute(query)
    rows = cursor.fetchall()
    conn.close()

    # Prepare data in Whisper-compatible format
    prepared_data = []
    for row in rows:
        audio_path, text, language = row
        prepared_data.append({
            "audio_filepath": audio_path,
            "text": text,
            "language_code": language
        })

    # Save the prepared data as a JSON file
    os.makedirs(f"{output_dir}/", exist_ok=True)
    training_data_path = os.path.join(output_dir, "training_data.json")
    with open(training_data_path, "w") as f:
        json.dump(prepared_data, f, indent=4)

    print(f"Prepared data saved to: {output_dir}")
    return training_data_path


if __name__ == "__main__":
    db_path = "data/assets/scenarios.db"
    prepared_data_path = prepare_whisper_data(db_path)
    print(f"Prepared dataset is available at: {prepared_data_path}")

```

Contents of apps/data_curation/prompts/__init__.py:
```

```

Contents of apps/data_curation/prompts/v1/__init__.py:
```

```

Contents of apps/training/__init__.py:
```

```

Contents of apps/training/main.py:
```
from fastapi import FastAPI, HTTPException
from shared.utils.logger import setup_logging
from shared.config.app_config import config
from pydantic import BaseModel
from apps.training.utils.training_utils import start_training

# Initialize the app
app = FastAPI(title=config.TRAINING_SERVICE_TITLE, version=config.API_VERSION)

# Setup logging
setup_logging()

class TrainingRequest(BaseModel):
    dataset_path: str
    model_name: str
    output_dir: str
    language_filter: str

# Root endpoint for health check
@app.get("/")
async def health_check():
    return {"status": "Training Service is running"}

@app.post("/train")
async def train_model(request: TrainingRequest):
    try:
        start_training(
            dataset_path=request.dataset_path,
            model_name=request.model_name,
            output_dir=request.output_dir,
            language_filter=request.language_filter
        )
        return {"status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

Contents of apps/training/utils/training_utils.py:
```
import json
import torch
from datasets import Dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
from torch.nn.utils.rnn import pad_sequence

# Optional bitsandbytes integration
# pip install bitsandbytes
try:
    from transformers import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
except ImportError:
    BITSANDBYTES_AVAILABLE = False


# -----------------------------
# 1. Detect device
# -----------------------------
def get_device():
    """
    Return the best available device: CUDA if available, else MPS if available, else CPU.
    """
    if torch.cuda.is_available():
        print("Using CUDA device.")
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        print("Using Apple Silicon MPS device.")
        return "mps"
    else:
        print("Using CPU device.")
        return "cpu"

DEVICE = get_device()


# -----------------------------
# 2. Whisper Data Collator
# -----------------------------
class WhisperDataCollator:
    def __init__(self, processor):
        self.processor = processor

    def __call__(self, features):
        input_features = [torch.tensor(f["input_features"]) for f in features]
        labels = [torch.tensor(f["labels"]) for f in features]

        # Pad the input_features
        input_features = pad_sequence(input_features, batch_first=True)

        # Pad the labels with the tokenizer pad token
        labels = pad_sequence(
            labels,
            batch_first=True,
            padding_value=self.processor.tokenizer.pad_token_id,
        )

        # Replace pad token with -100 for cross-entropy ignoring
        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        return {"input_features": input_features, "labels": labels}


# -----------------------------
# 3. Preprocessing function
# -----------------------------
def preprocess_batch(batch, processor, sampling_rate=16000):
    """
    Preprocess a batch of audio data for Whisper training.
    
    Args:
        batch: Dictionary containing audio_filepath, text, and language_code
        processor: WhisperProcessor instance for feature extraction and tokenization
        sampling_rate: Audio sampling rate in Hz (default: 16000)
    
    Returns:
        Dictionary containing preprocessed input_features and labels
    """
    input_features_list = []
    labels_list = []

    for audio, text, language_code in zip(
        batch["audio_filepath"], batch["text"], batch["language_code"]
    ):
        # Extract audio features
        input_features = processor.feature_extractor(
            audio["array"], sampling_rate=sampling_rate
        ).input_features[0]

        # Prepend language token
        language_token = f"<|{language_code}|>"
        labels = processor.tokenizer(
            f"{language_token} {text}", 
            return_tensors="pt"
        ).input_ids[0]

        input_features_list.append(input_features)
        labels_list.append(labels)

    return {"input_features": input_features_list, "labels": labels_list}


# -----------------------------
# 4. Training function
# -----------------------------
def start_training(
    dataset_path,
    model_name,
    output_dir,
    language_filter=None,
    use_8bit=False,
):
    """
    Fine-tune a Whisper model with options for device detection (cuda/mps/cpu) and optional 8-bit.
    """

    # Load JSON data
    with open(dataset_path, "r") as f:
        data = json.load(f)

    # Filter by language code if provided
    if language_filter:
        data = [item for item in data if item["language_code"] == language_filter]
        if not data:
            raise ValueError(f"No data found for language code: {language_filter}")
        print(f"Filtered dataset to {len(data)} examples for language code: {language_filter}")

    # Convert to HF Dataset
    dataset = Dataset.from_list(data)

    # Cast audio column to Audio feature
    sampling_rate = 16000
    dataset = dataset.cast_column("audio_filepath", Audio(sampling_rate=sampling_rate))

    # Decide if we want quantization
    # For Apple Silicon (MPS), bitsandbytes 8-bit is not supported, so skip it.
    quantization_config = None
    if use_8bit and BITSANDBYTES_AVAILABLE and DEVICE == "cuda":
        # Use 8-bit quantization
        print("Using bitsandbytes 8-bit quantization...")
        quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)
    else:
        if use_8bit and DEVICE != "cuda":
            print("Warning: 8-bit quantization only works on NVIDIA GPUs, skipping...")

    # Load processor
    processor = WhisperProcessor.from_pretrained(model_name)

    # Load model
    if quantization_config is not None:
        # This automatically places the model on the GPU with 8-bit weights
        model = WhisperForConditionalGeneration.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",  # let HF handle device mapping
        )
    else:
        # Normal load in full precision
        model = WhisperForConditionalGeneration.from_pretrained(model_name)
        # Move model to device manually if not using device_map='auto'
        model.to(DEVICE)

    # Preprocess dataset with processor and sampling_rate
    dataset = dataset.map(
        lambda x: preprocess_batch(x, processor, sampling_rate),
        batched=True,
        remove_columns=None  # Don't remove columns here
    )

    # Now create the final dataset with just the required columns
    dataset = Dataset.from_dict({
        'input_features': dataset['input_features'],
        'labels': dataset['labels']
    })

    # Data collator
    data_collator = WhisperDataCollator(processor)

    # Training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=1,  # simpler with a tiny dataset
        learning_rate=1e-5,
        num_train_epochs=3,
        logging_dir=f"{output_dir}/logs",
        logging_steps=1,
        save_steps=50,
        eval_steps=50,
        save_total_limit=2,
        # If you are on MPS, you can try bf16=True if supported:
        bf16=(DEVICE == "cuda"),  # or use bf16 on MPS if PyTorch supports it
        fp16=(DEVICE == "cuda"),  # typical for cuda
        # For MPS, do not set fp16=True. MPS uses a separate half mechanism.
        # If you want compile speedups on newer PyTorch, you can also try:
        # torch_compile=True
    )

    # Initialize the Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        tokenizer=processor.tokenizer,
    )

    # Train
    trainer.train()

    # Save model
    model.save_pretrained(output_dir)
    processor.save_pretrained(output_dir)
    print(f"Fine-tuning complete. Model and processor saved to: {output_dir}")


# -----------------------------
# 5. If running as script
# -----------------------------
if __name__ == "__main__":
    model_name = "openai/whisper-small"

    # Pre-download the model to avoid slow downloads in training loop
    print("Downloading model and processor if not already cached...")
    WhisperProcessor.from_pretrained(model_name)
    WhisperForConditionalGeneration.from_pretrained(model_name)
    print("Model and processor downloaded successfully.")

    # Example usage:
    start_training(
        dataset_path="data/training_data/training_data.json",
        model_name=model_name,
        output_dir="data/whisper_finetuned",
        language_filter="en",  # or None
        use_8bit=True,         # only works if you have a CUDA/NVIDIA GPU
    )
    print("Training complete.")



"""
{
  "dataset_path": "data/training_data/training_data.json",
  "model_name": "openai/whisper-small",
  "output_dir": "data/whisper_finetuned",
  "language_filter": "en"
}
"""
```

